<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="x-ua-compatible" content="ie=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>Ilya Prokin's Blog - Dimensionality reduction with correlated features</title>
        <link rel="stylesheet" href="../css/default.css" />
        <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
        </script>
        <link href="../css/prism.css" rel="stylesheet" />
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans" />
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu+Mono"> 
        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-129520228-1"></script>
        <script>
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());

          gtag('config', 'UA-129520228-1');
        </script>
    </head>
    <body>
        <script src="../scripts/prism.js"></script>
        <header>
            <nav>
                <a href="../">Home</a>
                <a href="../about.html">About</a>
                <a href="../contact.html">Contact</a>
                <a href="../archive.html">Blog Archive</a>
                <a href="../research.html">Research</a>
                <a href="../CV.html">CV</a>
            </nav>
        </header>

        <main role="main">
            <h1>Dimensionality reduction with correlated features</h1>
            <script type="text/javascript" async src="../scripts/disqus.js">
</script>
<article>
    <section class="header">
        Posted on August 26, 2018
        
            by Ilya
        
    </section>
    <section>
        <h1 id="intro-dimensionality-reduction-with-multicollinearity">Intro: dimensionality reduction with multicollinearity</h1>
<p>It is well known that one cannot meaningfully apply PCA when one has correlated features (see the discussion <a href="https://stats.stackexchange.com/questions/50537/should-one-remove-highly-correlated-variables-before-doing-pca">here</a>, see also <a href="https://en.wikipedia.org/wiki/Multicollinearity">multicollinearity</a>). How about new techniques such as <a href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding">t-SNE</a> or <a href="https://github.com/lmcinnes/umap">UMAP</a>?</p>
<p>We are going to find out experimentally.</p>
<h1 id="code-comparing-pca-t-sne-and-umap">Code: comparing PCA, t-SNE, and UMAP</h1>
<pre><code class="language-python">
```python
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
from umap import UMAP
import numpy as np
import matplotlib.pylab as plt

# Define data generating process
phi = np.linspace(0, 6*2*np.pi, 500)
r = 10
x = r*np.cos(phi) 
y = r*np.sin(phi)

for name, reducer in [ ('PCA' , PCA())
                     , ('UMAP', UMAP())
                     , ('TSNE', TSNE())
                     ]:

    # Make design matrix, simulate noisy measurement of the process above. with x first, y last
    X = np.vstack([x + np.random.randn(len(x))] + \
                  [y + np.random.randn(len(y))]).T

    _, axs = plt.subplots(1, 2, figsize=(4*2, 4))

    axs[0].scatter(X[:, 0], X[:, -1])

    for n_x in [1, 10, 100]:
        if n_x > 1:
            X_with_corr = np.hstack(
                [ np.vstack(
                    [ x + np.random.randn(len(x))
                      for _ in range(n_x-1)
                    ]).T
                , X
                ])
        else:
            X_with_corr = X

        X_tr = reducer.fit_transform(X_with_corr)

        axs[1].scatter(X_tr[:, 0], X_tr[:, -1],
                       label="n_x={}".format(n_x))
        axs[1].legend()

    axs[0].set_title(name)
```
</code></pre>
<p><img src="../images/dimensionality-reduction-with-multicollinearity/PCA.png" /> <img src="../images/dimensionality-reduction-with-multicollinearity/UMAP.png" /> <img src="../images/dimensionality-reduction-with-multicollinearity/TSNE.png" /></p>
<h1 id="conclusion-when-pca-and-t-sne-fail-umap-can-do-well">Conclusion: when PCA and t-SNE fail, UMAP can do well</h1>
<p>First observation, PCA fails miserably. t-SNE did ok when a feature was repeated 10 times, but did badly when it was repeated 100 times. UMAP however did pretty well even with very high multicollinearity (100 repeat). At least, for in this specific experiment, UMAP is a clear winner.</p>
    </section>
    <div id="disqus_thread"></div>
</article>

        </main>

        <footer>
            Site proudly generated by
            <a href="http://jaspervdj.be/hakyll">Hakyll</a>
        </footer>
    </body>
</html>
